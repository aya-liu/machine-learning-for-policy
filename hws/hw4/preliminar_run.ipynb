{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from dateutil import parser\n",
    "\n",
    "import pipeline as pp\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "file = 'data/projects_2012_2013.csv'\n",
    "coltypes = {'school_ncesid': str}\n",
    "parse_dates = ['date_posted', 'datefullyfunded']\n",
    "df = pp.read_csv(file, coltypes=coltypes, parse_dates=parse_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "\n",
    "## Generate outcome variable\n",
    "\n",
    "# Calculate the number of days it takes for a project to get fully funded\n",
    "df['time_till_funded'] = (df.datefullyfunded - df.date_posted).apply(lambda x: x.days)\n",
    "# Create target variable to identify projects fully funded within 60 days of their posting dates\n",
    "df['not_funded_wi_60d'] = np.where(df.time_till_funded > 60, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct pipeline\n",
    "pipeline = pp.Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pipeline parameters\n",
    "\n",
    "label = 'not_funded_wi_60d'\n",
    "predictor_sets = [['school_city', 'school_state',\n",
    "       'school_metro', 'school_district', 'school_county', 'school_charter',\n",
    "       'school_magnet', 'teacher_prefix', 'primary_focus_subject',\n",
    "       'primary_focus_area', 'secondary_focus_subject', 'secondary_focus_area',\n",
    "       'resource_type', 'poverty_level', 'grade_level',\n",
    "       'total_price_including_optional_support', 'students_reached',\n",
    "       'eligible_double_your_impact_match']]\n",
    "time_col = 'date_posted'\n",
    "start = parser.parse('2012-01-01')\n",
    "end = parser.parse('2013-12-31')\n",
    "test_window_months = 6\n",
    "outcome_lag_days = 60\n",
    "output_dir = 'output'\n",
    "output_filename = 'evaluations.csv'\n",
    "grid_size='test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "set up done. output: output/evaluations.csv\n",
      "## TRAIN: 2012-01-01 00:00:00 - 2012-06-30 00:00:00, TEST:2012-08-30 00:00:00 - 2013-02-27 00:00:00 ##\n",
      "### Predictors: ['school_city', 'school_state', 'school_metro', 'school_district', 'school_county', 'school_charter', 'school_magnet', 'teacher_prefix', 'primary_focus_subject', 'primary_focus_area', 'secondary_focus_subject', 'secondary_focus_area', 'resource_type', 'poverty_level', 'grade_level', 'total_price_including_optional_support', 'students_reached', 'eligible_double_your_impact_match']\n",
      "...train test split done\n",
      "...pre-processing done\n",
      "...feature generation done\n",
      "#### RF\n",
      "\t{'max_depth': 1, 'max_features': 'sqrt', 'min_samples_split': 10, 'n_estimators': 1, 'n_jobs': -1}\n",
      "set params\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=1, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=10,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "generated prediction scores\n",
      "stored feature importances\n",
      "\t---model results saved---\n",
      "#### ET\n",
      "\t{'criterion': 'gini', 'max_depth': 1, 'max_features': 'sqrt', 'min_samples_split': 10, 'n_estimators': 1, 'n_jobs': -1}\n",
      "set params\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=1, max_features='sqrt', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=10,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=1, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "generated prediction scores\n",
      "stored feature importances\n",
      "\t---model results saved---\n",
      "#### AB\n",
      "\t{'algorithm': 'SAMME', 'n_estimators': 1}\n",
      "set params\n",
      "AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=1.0, n_estimators=1, random_state=None)\n",
      "generated prediction scores\n",
      "stored feature importances\n",
      "\t---model results saved---\n",
      "#### LR\n",
      "\t{'C': 0.01, 'penalty': 'l1'}\n",
      "set params\n",
      "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l1', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "generated prediction scores\n",
      "stored feature importances\n",
      "\t---model results saved---\n",
      "#### GB\n",
      "\t{'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1, 'subsample': 0.5}\n",
      "set params\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=1,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=1,\n",
      "              n_iter_no_change=None, presort='auto', random_state=None,\n",
      "              subsample=0.5, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False)\n",
      "generated prediction scores\n",
      "stored feature importances\n",
      "\t---model results saved---\n",
      "#### NB\n",
      "\t{}\n",
      "set params\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "generated prediction scores\n",
      "\t---model results saved---\n",
      "#### DT\n",
      "\t{'max_depth': 1, 'min_samples_split': 10}\n",
      "set params\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=10,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "generated prediction scores\n",
      "stored feature importances\n",
      "\t---model results saved---\n",
      "#### KNN\n",
      "\t{'algorithm': 'auto', 'n_neighbors': 5, 'weights': 'uniform'}\n",
      "set params\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n"
     ]
    }
   ],
   "source": [
    "pipeline.run(df, time_col, predictor_sets, label, start, end, test_window_months, \n",
    "            outcome_lag_days, output_dir, output_filename, grid_size='test', thresholds=[], \n",
    "            ks=list(np.arange(0, 1, 0.1)), save_output=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=50, n_jobs=-1)\n",
    "rf.set_params(**{'max_depth': 1, 'max_features': 'sqrt', 'min_samples_split': 10, 'n_estimators': 1, 'n_jobs': -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RF': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=-1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       " 'ET': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       " 'AB': AdaBoostClassifier(algorithm='SAMME',\n",
       "           base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       "           learning_rate=1.0, n_estimators=200, random_state=None),\n",
       " 'LR': LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "           multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
       "           solver='warn', tol=0.0001, verbose=0, warm_start=False),\n",
       " 'GB': GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "               learning_rate=0.05, loss='deviance', max_depth=6,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "               n_iter_no_change=None, presort='auto', random_state=None,\n",
       "               subsample=0.5, tol=0.0001, validation_fraction=0.1,\n",
       "               verbose=0, warm_start=False),\n",
       " 'NB': GaussianNB(priors=None, var_smoothing=1e-09),\n",
       " 'DT': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "             splitter='best'),\n",
       " 'KNN': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "            metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "            weights='uniform')}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.clfs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlpp",
   "language": "python",
   "name": "other-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
