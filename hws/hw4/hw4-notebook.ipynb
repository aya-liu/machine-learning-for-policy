{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    " \n",
    "Aya Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code from this notebook is included in `main.py` to run in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from dateutil import parser\n",
    "\n",
    "import data_preprocess as preprocess\n",
    "import pipeline as pp\n",
    "\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "In the pipeline, I ran various models on three training sets to predict whether an educational project is not fully funded within 60 days. Predictions made from the model can be used to target educational projects that are lacking timely funding and provide them with resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "file = 'data/projects_2012_2013.csv'\n",
    "coltypes = {'school_ncesid': str}\n",
    "parse_dates = ['date_posted', 'datefullyfunded']\n",
    "df = pp.read_csv(file, coltypes=coltypes, parse_dates=parse_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "df = preprocess.pre_pipeline_clean(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct pipeline\n",
    "pipeline = pp.Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pipeline parameters\n",
    "label = 'not_funded_wi_60d'\n",
    "predictor_sets = [['school_city', 'school_state',\n",
    "       'school_metro', 'school_district', 'school_county', 'school_charter',\n",
    "       'school_magnet', 'teacher_prefix', 'primary_focus_subject',\n",
    "       'primary_focus_area', 'secondary_focus_subject', 'secondary_focus_area',\n",
    "       'resource_type', 'poverty_level', 'grade_level',\n",
    "       'total_price_including_optional_support', 'students_reached',\n",
    "       'eligible_double_your_impact_match']]\n",
    "time_col = 'date_posted'\n",
    "start = parser.parse('2012-01-01')\n",
    "end = parser.parse('2013-12-31')\n",
    "test_window_months = 6\n",
    "outcome_lag_days = 60\n",
    "output_dir = 'output_test_053019'\n",
    "output_filename = 'evaluations.csv'\n",
    "ks = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "GRID SIZE = test\n",
      "set up done. output: output_small_053019/evaluations.csv\n",
      "## TRAIN: 2012-01-01 00:00:00 - 2012-06-30 00:00:00, TEST:2012-08-30 00:00:00 - 2013-02-27 00:00:00 ##\n",
      "### Predictors: ['school_city', 'school_state', 'school_metro', 'school_district', 'school_county', 'school_charter', 'school_magnet', 'teacher_prefix', 'primary_focus_subject', 'primary_focus_area', 'secondary_focus_subject', 'secondary_focus_area', 'resource_type', 'poverty_level', 'grade_level', 'total_price_including_optional_support', 'students_reached', 'eligible_double_your_impact_match']\n",
      "...train test split done\n",
      "...pre-processing done\n",
      "...feature generation done\n",
      "#### 0-0: RF\n",
      "{'max_depth': 1, 'max_features': 'sqrt', 'min_samples_split': 10, 'n_estimators': 1, 'n_jobs': -1}\n",
      "---model results saved---\n",
      "#### 0-1: ET\n",
      "{'criterion': 'gini', 'max_depth': 1, 'max_features': 'sqrt', 'min_samples_split': 10, 'n_estimators': 1, 'n_jobs': -1}\n",
      "---model results saved---\n",
      "#### 0-2: AB\n",
      "{'algorithm': 'SAMME', 'n_estimators': 1}\n",
      "---model results saved---\n",
      "#### 0-3: LR\n",
      "{'C': 0.01, 'penalty': 'l1'}\n",
      "---model results saved---\n",
      "#### 0-4: GB\n",
      "{'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1, 'subsample': 0.5}\n",
      "---model results saved---\n",
      "#### 0-5: NB\n",
      "{}\n",
      "---model results saved---\n",
      "#### 0-6: DT\n",
      "{'max_depth': 1, 'min_samples_split': 10}\n",
      "---model results saved---\n",
      "## TRAIN: 2012-01-01 00:00:00 - 2012-12-29 00:00:00, TEST:2013-02-28 00:00:00 - 2013-08-27 00:00:00 ##\n",
      "### Predictors: ['school_city', 'school_state', 'school_metro', 'school_district', 'school_county', 'school_charter', 'school_magnet', 'teacher_prefix', 'primary_focus_subject', 'primary_focus_area', 'secondary_focus_subject', 'secondary_focus_area', 'resource_type', 'poverty_level', 'grade_level', 'total_price_including_optional_support', 'students_reached', 'eligible_double_your_impact_match']\n",
      "...train test split done\n",
      "...pre-processing done\n",
      "...feature generation done\n",
      "#### 1-0: RF\n",
      "{'max_depth': 1, 'max_features': 'sqrt', 'min_samples_split': 10, 'n_estimators': 1, 'n_jobs': -1}\n",
      "---model results saved---\n",
      "#### 1-1: ET\n",
      "{'criterion': 'gini', 'max_depth': 1, 'max_features': 'sqrt', 'min_samples_split': 10, 'n_estimators': 1, 'n_jobs': -1}\n",
      "---model results saved---\n",
      "#### 1-2: AB\n",
      "{'algorithm': 'SAMME', 'n_estimators': 1}\n",
      "---model results saved---\n",
      "#### 1-3: LR\n",
      "{'C': 0.01, 'penalty': 'l1'}\n",
      "---model results saved---\n",
      "#### 1-4: GB\n",
      "{'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1, 'subsample': 0.5}\n",
      "---model results saved---\n",
      "#### 1-5: NB\n",
      "{}\n",
      "---model results saved---\n",
      "#### 1-6: DT\n",
      "{'max_depth': 1, 'min_samples_split': 10}\n",
      "---model results saved---\n",
      "## TRAIN: 2012-01-01 00:00:00 - 2013-06-28 00:00:00, TEST:2013-08-28 00:00:00 - 2013-11-01 00:00:00 ##\n",
      "### Predictors: ['school_city', 'school_state', 'school_metro', 'school_district', 'school_county', 'school_charter', 'school_magnet', 'teacher_prefix', 'primary_focus_subject', 'primary_focus_area', 'secondary_focus_subject', 'secondary_focus_area', 'resource_type', 'poverty_level', 'grade_level', 'total_price_including_optional_support', 'students_reached', 'eligible_double_your_impact_match']\n",
      "...train test split done\n",
      "...pre-processing done\n",
      "...feature generation done\n",
      "#### 2-0: RF\n",
      "{'max_depth': 1, 'max_features': 'sqrt', 'min_samples_split': 10, 'n_estimators': 1, 'n_jobs': -1}\n",
      "---model results saved---\n",
      "#### 2-1: ET\n",
      "{'criterion': 'gini', 'max_depth': 1, 'max_features': 'sqrt', 'min_samples_split': 10, 'n_estimators': 1, 'n_jobs': -1}\n",
      "---model results saved---\n",
      "#### 2-2: AB\n",
      "{'algorithm': 'SAMME', 'n_estimators': 1}\n",
      "---model results saved---\n",
      "#### 2-3: LR\n",
      "{'C': 0.01, 'penalty': 'l1'}\n",
      "---model results saved---\n",
      "#### 2-4: GB\n",
      "{'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 1, 'subsample': 0.5}\n",
      "---model results saved---\n",
      "#### 2-5: NB\n",
      "{}\n",
      "---model results saved---\n",
      "#### 2-6: DT\n",
      "{'max_depth': 1, 'min_samples_split': 10}\n",
      "---model results saved---\n",
      "FINISH\n"
     ]
    }
   ],
   "source": [
    "# Run pipeline on test grid\n",
    "pipeline.run(df, time_col, predictor_sets, label, start, end, test_window_months, \n",
    "            outcome_lag_days, output_dir, output_filename, grid_size='test', thresholds=[], \n",
    "            ks=ks, save_output=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on grid\n",
    "\n",
    "I wasn't able to finish running the small grid on a RCC cluster -- I could only build 33 models in 4 hours and it was taking a very long time on GradientBoosting. Therefore, the analysis below is only done using the output generated by the test grid. I am aware that the results are not meaningful, but it shows the processes I will take to evaluate the small grid output if I had them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and test sets\n",
    "\n",
    "Each test set is 6 months long (except for the latest one). Between each training and test set there is a 60-day gap, because it takes 60 days to know whether the project posted on the last day of the training set time is fully funded within 60 days or not. That is also the reason why the last test set is 60 days earlier than our data end date 2013-12-31."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 0\n",
      "TRAIN: START 2012-01-01 00:00:00 END 2012-06-30 00:00:00\n",
      "TEST: START 2012-08-30 00:00:00 END 2013-02-27 00:00:00\n",
      "\n",
      "N = 1\n",
      "TRAIN: START 2012-01-01 00:00:00 END 2012-12-29 00:00:00\n",
      "TEST: START 2013-02-28 00:00:00 END 2013-08-27 00:00:00\n",
      "\n",
      "N = 2\n",
      "TRAIN: START 2012-01-01 00:00:00 END 2013-06-28 00:00:00\n",
      "TEST: START 2013-08-28 00:00:00 END 2013-11-01 00:00:00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (train_start, train_end, test_start, test_end) in enumerate(pipeline.train_test_times):\n",
    "    print('N = {}'.format(i))\n",
    "    print('TRAIN: START {} END {}'.format(train_start, train_end))\n",
    "    print('TEST: START {} END {}\\n'.format(test_start, test_end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalutaion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pipeline` contains functions to compare model performances.\n",
    "\n",
    "\n",
    "We mainly care about picking the model with the highest precision given the resource constraint of our intervention, because we want our limited resources to have a high \"hit rate\" for underfunded projects . \n",
    "\n",
    "\n",
    "\n",
    "### Compare precision when we can intervene with 10% most at-risk projects\n",
    "\n",
    "Given that we can only intervene with 10% of posted projects, we would want the model with the highest precision when we consider only the projects with the 10% highest risk scores as projects that will not be fully funded within 60 days, which are the projects receiving our assistance.\n",
    "\n",
    "The graph below tells us, among the models we've built, model 0-3, 1-3, and 2-3 (Logistic Regression with C=0.01, penalty=l1) has the highest precision (40%-50%), among which the model trained with the 2nd training set has the highest precision. This means if we use the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_filepath = output_dir + '/evaluations.csv' \n",
    "k = 0.1\n",
    "\n",
    "pp.compare_model_precisions_at_k(evaluations_filepath, k, save_output=True, \n",
    "         output_filepath='output_test_053019/compare_precision_at_0.1.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"output_test_053019/compare_precision_at_0.1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Average precision of model #3 Logistic Regression (C=0.01, Penalty=L1) from each train/test split is 0.45. That is, on average, 45% of the underfunded projects predicted by this model are actually not fully funded within 60 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4462279730250434"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev = pd.read_csv(evaluations_filepath)\n",
    "ev[(ev['i'] == 3) & (ev['k'] == 0.1)]['precision'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare overall performance of models regardless of intervention resource constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't need to adhere to a budget constraint that can only help a certain percentage of the population, we can use the model that has the best overall performance. For this, we look\n",
    " at models with the highest AUC scores. These models have the smallest overall false positive rates and false negative rates when we vary thresholds of risk scores above which we consider as underfunded.\n",
    " \n",
    "The graph below tells us, among the models we've built, model 0-3, 1-3, and 2-3 (Logistic Regression with C=0.01, penalty=l1) has the highest AUC score. This is the same model as the one with the highest precision at 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.compare_model_aucs(evaluations_filepath, save_output=True, \n",
    "                      output_filepath='output_test_053019/compare_auc.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"output_test_053019/compare_auc.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Curve for Model 1-3 at k\n",
    "Projects with the top k(%) highest probability scores of being underfunded are considered to be not fully funded within 60 days."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"output_test_053019/precision-recall_1-3_LR.png\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlpp",
   "language": "python",
   "name": "other-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
